{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 11. Time Series Analysis**\n",
        "\n",
        "## 1. Definition and Importance of Time Series Analysis\n",
        "- Time series data is a sequence of observations recorded at regular time intervals. This type of analysis is crucial for understanding underlying patterns to forecast future values.\n",
        "- Applications include predicting stock market trends, weather forecasting, energy demand forecasting, and analyzing business metrics like sales and website traffic.\n",
        "\n",
        "## 2. Components of Time Series\n",
        "- **Trend**: Long-term movement in data over time, either up or down.\n",
        "- **Seasonality**: Patterns that repeat at regular intervals, such as weekly, monthly, or quarterly.\n",
        "- **Cyclical Patterns**: Fluctuations occurring at irregular intervals, influenced by economic or other external factors.\n",
        "- **Irregularity (Noise)**: Random variation in the series.\n",
        "\n",
        "## 3. Python Libraries Overview\n",
        "- **pandas** for data manipulation and analysis.\n",
        "- **NumPy** for numerical computing.\n",
        "- **matplotlib** for plotting graphs.\n",
        "- **statsmodels** for implementing statistical models.\n",
        "\n",
        "## 4. Basics of Handling Time Series Data in Python\n",
        "\n",
        "### Time Series Data Structures\n",
        "- pandas `DateTimeIndex` for handling dates and times.\n",
        "- Conversion functions like `pd.to_datetime()` for converting string dates to `datetime` objects.\n",
        "\n",
        "### Indexing time series data using dates\n",
        "```python\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "filedir = '/content/drive/MyDrive/Teaching/FWE458_Spring2025/Lec9/'\n",
        "fname = filedir + \"COVID19-Historical-V2-ST.csv\"\n",
        "\n",
        "covid = pd.read_csv(fname)\n",
        "covid = covid[[\"Date\", \"POS_NEW_CP\"]]\n",
        "\n",
        "# date index\n",
        "covid['Date'] = pd.to_datetime(covid['Date'])\n",
        "covid = covid.set_index('Date')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 6))\n",
        "ax.plot(covid.POS_NEW_CP, marker='.', linestyle='-', linewidth=0.5, label='daily')\n",
        "ax.set_ylabel('New Positive Cases')\n",
        "ax.legend()\n",
        "```\n",
        "\n",
        "### Slicing data to obtain specific time periods\n",
        "``` python\n",
        "covid = pd.read_csv(fname)\n",
        "covid['Date'] = pd.to_datetime(covid['Date'])\n",
        "covid = covid.set_index('Date')\n",
        "\n",
        "# slicing data\n",
        "covid = covid.loc['2020-04-01':'2021-06-15']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 6))\n",
        "ax.plot(covid.POS_NEW_CP, marker='.', linestyle='-', linewidth=0.5, label='daily')\n",
        "ax.set_ylabel('New Positive Cases')\n",
        "ax.legend()\n",
        "```\n",
        "\n",
        "### Aggregating data over time using `resample()`\n",
        "``` python\n",
        "covid = pd.read_csv(fname)\n",
        "covid['Date'] = pd.to_datetime(covid['Date'])\n",
        "covid = covid.set_index('Date')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 6))\n",
        "ax.plot(covid.POS_NEW_CP, marker='.', linestyle='-', linewidth=0.5, label='daily')\n",
        "ax.plot(covid.POS_NEW_CP.resample('W').mean(),marker='o', markersize=8, linestyle='-', label='Weekly Resample')\n",
        "ax.plot(covid.POS_NEW_CP.resample('M').mean(),marker='o', markersize=8, linestyle='-', label='Monthly Resample')\n",
        "ax.plot(covid.POS_NEW_CP.resample('Y').mean(),marker='o', markersize=8, linestyle='-', label='Yearly Resample')\n",
        "\n",
        "ax.set_ylabel('New Positive Cases')\n",
        "ax.legend()\n",
        "```\n",
        "\n",
        "### Moving average\n",
        "```python\n",
        "# Moving average\n",
        "fig, ax = plt.subplots(figsize=(20, 6))\n",
        "ax.plot(covid.POS_NEW_CP, 'k.', label='daily')\n",
        "\n",
        "# simple moving average (SMA)\n",
        "ax.plot(covid.POS_NEW_CP.rolling(7).mean(), linestyle='-', linewidth=3, label='7-day moving average')\n",
        "\n",
        "# Exponential Moving Average (EMA) - weights added\n",
        "ax.plot(covid.POS_NEW_CP.ewm(7).mean(), linestyle='-', linewidth=3, label='20-day moving average')\n",
        "\n",
        "# An Exponential Moving Average (EMA) is a weighted moving average that emphasizes recent data points more heavily than older ones by applying an exponentially decreasing weight over time. This makes the EMA more responsive to recent price changes or fluctuations in the data compared to a simple moving average (SMA). The calculation uses a smoothing factor, commonly defined as α = 2/(N + 1) for a chosen period N, which determines how quickly the weights decrease for older data.\n",
        "\n",
        "# Cumulative Moving Average (EMA) - the width of the window increases as duration increases\n",
        "ax.plot(covid.POS_NEW_CP.expanding(7).mean(), linestyle='-', linewidth=3, label='20-day moving average')\n",
        "\n",
        "# A Cumulative Moving Average (CMA) calculates the average of all data points up to the current time, updating with each new observation by giving equal weight to all past values, which offers a stable overall trend but can be slow to reflect recent changes.\n",
        "\n",
        "ax.set_ylabel('New Positive Cases')\n",
        "ax.legend()\n",
        "```\n",
        "\n",
        "## **4. Time Series Analysis Techniques**\n",
        "\n",
        "### Time series decomposition\n",
        "Time series decomposition is a statistical method that deconstructs a time series into several components, typically including trend, seasonality, and residual (or irregular) components. This method is crucial for understanding the underlying patterns in the data, which can be particularly useful for forecasting. The `statsmodels` library in Python provides efficient tools for performing time series decomposition.\n",
        "\n",
        "![picture](https://www.bounteous.com/sites/default/files/b_inline_20200914.png)\n",
        "\n",
        "### Components of Time Series Decomposition\n",
        "\n",
        "1. **Trend**: The trend component reflects the long-term progression of the series, showing an upward or downward movement over time. It represents the increase or decrease in the data's value over a long period.\n",
        "\n",
        "2. **Seasonality**: Seasonality shows fluctuations that occur at specific regular intervals less than a year, such as monthly or quarterly. This component is useful for understanding repetitive patterns over fixed periods.\n",
        "\n",
        "3. **Residual (Irregular or Noise)**: The residual component contains the randomness or irregular movements in the time series, not explained by the trend or seasonal components. It's essentially the remainder of the time series after the trend and seasonal components have been removed.\n",
        "\n",
        "### Decomposition with statsmodels\n",
        "\n",
        "`statsmodels` offers two main methods for time series decomposition: **additive** and **multiplicative**.\n",
        "\n",
        "- **Additive Model**: Used when the seasonal variations are roughly constant over time. The observed time series is the sum of the trend, seasonal, and residual components.\n",
        "  \n",
        "  Yt = Tt+St+Rt\n",
        "\n",
        "- **Multiplicative Model**: Used when the seasonal variations change proportionally to the level of the trend. The observed time series is the product of the trend, seasonal, and residual components.\n",
        "  \n",
        "  Yt = Tt\\*St\\*Rt\n",
        "\n",
        "#### Examples of Time Series Decomposition Using statsmodels\n",
        "\n",
        "```python\n",
        "import statsmodels.api as sm\n",
        "\n",
        "decomposition = sm.tsa.seasonal_decompose(covid.POS_NEW_CP, model='additive', period=365)\n",
        "fig = decomposition.plot()\n",
        "fig.set_size_inches(14,7)\n",
        "```\n",
        "\n",
        "```python\n",
        "# do decomposition with the Maula Loa CO2 data\n",
        "drive.mount('/content/drive')\n",
        "filedir = '/content/drive/MyDrive/Teaching/FWE458_Spring2025/Lec11/'\n",
        "fname = filedir + \"co2_mm_mlo.csv\"\n",
        "\n",
        "#filedir = '/content/drive/MyDrive/Teaching/FWE458_Spring2025/Lec9/'\n",
        "#fname = filedir + \"COVID19-Historical-V2-ST.csv\"\n",
        "\n",
        "#covid = pd.read_csv(fname)\n",
        "#covid = covid[[\"Date\", \"POS_NEW_CP\"]]\n",
        "\n",
        "co2 = pd.read_csv(fname, parse_dates= {\"date\" : [\"year\",\"month\"]}, comment=\"#\")\n",
        "co2 = co2.set_index(\"date\")\n",
        "#co2\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 6))\n",
        "ax.plot(co2.average, marker='.', linestyle='-', linewidth=0.5, label='daily')\n",
        "\n",
        "decomposition = sm.tsa.seasonal_decompose(co2.average, model='additive', period=12)\n",
        "fig = decomposition.plot()\n",
        "fig.set_size_inches(14,7)\n",
        "\n",
        "```\n",
        "\n",
        "## Stationary vs Non-Stationary Time series\n",
        "\n",
        "In time series analysis, distinguishing between stationary and non-stationary processes is fundamental. The concepts of stationarity and non-stationarity have profound implications for modeling and forecasting time series data.\n",
        "\n",
        "### Stationary Time Series\n",
        "\n",
        "A stationary time series is one whose properties do not depend on the time at which the series is observed. Thus, it doesn't matter when you observe a stationary series; its basic properties, like mean, variance, and autocorrelation, are constant over time. Stationarity is an important assumption in many time series analysis methods, as it implies that the time series is predictable and exhibits a regular structure over time.\n",
        "\n",
        "**Characteristics of Stationary Time Series:**\n",
        "- **Constant Mean:** The average value of the series does not change over time.\n",
        "- **Constant Variance:** The variability of the series remains the same over time.\n",
        "- **Constant Autocorrelation Structure:** How the series correlates with itself over different intervals does not change over time.\n",
        "- **No Periodic Fluctuations (Seasonality):** While some stationary series might exhibit cyclic behavior, where the cycles are not of a fixed length, true seasonality violates stationarity.\n",
        "\n",
        "**Testing for Stationarity:**\n",
        "Tools like the Augmented Dickey-Fuller (ADF) test, KPSS test, and Phillips-Perron test are commonly used to statistically test for stationarity in a time series.\n",
        "\n",
        "### Non-stationary Time Series\n",
        "\n",
        "A non-stationary time series is one whose statistical properties change over time. This can manifest as a change in mean over time, a change in variance, or a change in other aspects of the series' distribution. Non-stationary data are more challenging to model and forecast due to their changing structure.\n",
        "\n",
        "**Characteristics of Non-Stationary Time Series:**\n",
        "- **Variable Mean:** The average value of the series may trend upwards or downwards over time.\n",
        "- **Variable Variance:** The variability of the series may increase or decrease over time.\n",
        "- **Variable Autocorrelation Structure:** The way in which the series' values are correlated with themselves over time may change.\n",
        "- **Presence of Seasonality or Trends:** Non-stationary series often exhibit trends (either deterministic or stochastic) and/or seasonality.\n",
        "\n",
        "Non-stationary time series analysis often involves making the series stationary before modeling. This is typically done through differencing the series, transforming the data (e.g., logarithmic transformation), or detrending.\n",
        "\n",
        "### Why Stationarity Matters\n",
        "\n",
        "Many statistical forecasting methods and models (like ARIMA) assume or require the time series to be stationary to make accurate predictions. This is because stationary series have a consistent structure that can be learned and predicted over time, whereas non-stationary series do not, making their future values more unpredictable with those models.\n",
        "\n",
        "In practice, dealing with non-stationarity is a common task in time series analysis, and identifying whether a series is stationary or non-stationary is a critical first step in selecting the appropriate modeling approach.\n",
        "\n",
        "#### An Everyday Analogy\n",
        "Think of it like baking a cake with a recipe:\n",
        "\n",
        "##### Stationary Data:\n",
        "Imagine you have a fixed recipe. Every time you bake the cake, you use the same amount of flour, sugar, and eggs. Your cakes turn out similar because the ingredients and the process are consistent.\n",
        "\n",
        "##### Non-Stationary Data:\n",
        "Now, imagine if every time you baked a cake, someone changed the recipe—sometimes more sugar, sometimes less flour. Predicting the taste of the cake becomes much harder because the recipe keeps changing.\n",
        "\n",
        "In time series analysis, stationarity means the \"recipe\" (the underlying process of the data) is consistent. Without it, you're trying to predict something with shifting rules, which is much more difficult and can lead to incorrect conclusions.\n",
        "\n",
        "\n",
        "\n",
        "### Example for testing time series stationarity\n",
        "```python\n",
        "# ADF test\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "result = adfuller(co2.average)\n",
        "print('ADF Statistic: %f' % result[0])\n",
        "print('p-value: %f' % result[1]) # if p> 0.05 Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n",
        "\n",
        "#KPSS test\n",
        "from statsmodels.tsa.stattools import kpss\n",
        "result = kpss(co2.average)\n",
        "print('KPSS Statistic: %f' % result[0])\n",
        "print('p-value: %f' % result[1]) # if p < 0.05 non-stationary.\n",
        "```\n",
        "\n",
        "### Make your timeseries stationary\n",
        "Making a time series stationary is a crucial step in time series analysis, especially for models that assume stationarity, such as ARIMA (Autoregressive Integrated Moving Average). A stationary time series has constant mean and variance over time, and its autocovariance does not depend on time. Here are several methods to transform a non-stationary time series into a stationary one:\n",
        "\n",
        "#### 1. Differencing\n",
        "\n",
        "Differencing is one of the most common methods to stabilize the mean of a time series by removing changes in the level of a time series, thus eliminating (or reducing) trend and seasonality.\n",
        "\n",
        "- **First Differencing**: This involves subtracting the previous observation from the current observation. If the original time series is represented by $Y_t$, then the first-differenced series is $Y_t' = Y_t - Y_{t-1}$.\n",
        "- **Seasonal Differencing**: If the series has a seasonal pattern, then seasonal differencing can be effective. This involves subtracting the observation from the same season in the previous cycle, e.g., $Y_t' = Y_t - Y_{t-n}$, where $n$ is the seasonality period.\n",
        "\n",
        "#### 2. Transformation\n",
        "\n",
        "Transformations such as logarithmic, square root, or power transformations can help stabilize the variance of a time series.\n",
        "\n",
        "- **Log Transformation**: Applying a log transformation, $Y_t' = \\log(Y_t)$, can help stabilize a growing variance.\n",
        " - Use a log transformation when your time series shows exponential growth or increasing variance, as it stabilizes the variance and linearizes multiplicative relationships, making the data more stationary.\n",
        "- **Square Root Transformation**: The square root, $Y_t' = \\sqrt{Y_t}$, can also be effective for stabilizing variance.\n",
        " - Use a square root transformation when your time series data involves count values or exhibits moderate heteroscedasticity, as it helps stabilize variance without compressing data as aggressively as a log transformation.\n",
        "- **Box-Cox Transformation**: A more generalized approach that can stabilize variance and make the series more normal (Gaussian). The Box-Cox transformation requires a parameter, $\\lambda$, and is defined as $Y_t'(\\lambda) = \\frac{Y_t^\\lambda - 1}{\\lambda}$ for $\\lambda \\neq 0$, and $Y_t' = \\log(Y_t)$ for $\\lambda = 0$.\n",
        " - Use a Box-Cox transformation when your strictly positive time series data exhibits heteroscedasticity or non-normality, as it finds the optimal power parameter to stabilize variance and approximate normality.\n",
        "\n",
        "#### 3. Detrending\n",
        "\n",
        "Detrending involves removing the underlying trend in the series. This can be achieved by:\n",
        "\n",
        "- **Subtracting the Trend Line**: Fit a regression model to the time series and subtract the resulting trend line from the original series.\n",
        "- **Moving Average**: Subtracting the moving average of a certain period can also help remove the trend, making the series more stationary.\n",
        "\n",
        "#### 4. Decomposition\n",
        "\n",
        "Time series decomposition involves separating the time series into trend, seasonal, and residual components. Once decomposed, you can reconstruct the series without the trend and/or seasonal components, potentially leaving a stationary residual component.\n",
        "\n",
        "#### Example\n",
        "\n",
        "```python\n",
        "# detrending\n",
        "y = co2.average\n",
        "y_detrend = (y - y.rolling(window=12).mean())/y.rolling(window=12).std()\n",
        "\n",
        "y_detrend.plot()\n",
        "y_detrend.dropna(inplace=True)\n",
        "result = ADF_test(y_detrend,'de-trended data')\n",
        "\n",
        "# differencing\n",
        "\n",
        "# This method removes the underlying seasonal or cyclical patterns in the time series.\n",
        "y_12lag =  y - y.shift(12)\n",
        "y_12lag.dropna(inplace=True)\n",
        "y_12lag.plot()\n",
        "ADF_test(y_12lag, '12 lag differenced data')\n",
        "\n",
        "# Detrending + Differencing\n",
        "\n",
        "y_12lag_detrend =  y_detrend - y_detrend.shift(12)\n",
        "y_12lag_detrend.plot()\n",
        "ADF_test(y_12lag_detrend,'12 lag differenced de-trended data')\n",
        "```\n",
        "\n",
        "Transforming a non-stationary time series into a stationary one is essential for effective modeling and forecasting. The choice of method depends on the specific characteristics of the time series, such as the presence of trends or seasonality. Often, a combination of methods is used to achieve stationarity. After transforming the series, it's important to test for stationarity using statistical tests, such as the Augmented Dickey-Fuller test, to confirm that the transformation was successful.\n",
        "\n",
        "\n",
        "### Auto Regressive Modeling\n",
        "An AR model is a Linear Regression model, that uses lagged variables as input.\n",
        "\\begin{align}\n",
        "        Y_t =C+b_1Y_{t-1}+b_2Y_{t-2}+…+b_pY_{t-p}+Er_t\n",
        "    \\end{align}\n",
        "\n",
        "- $p$=past values\n",
        "- $Y_t$=Function of different past values\n",
        "- $Er_t$=errors in time\n",
        "- $C$=intercept\n",
        "\n",
        "```python\n",
        "# implementing AR model\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "\n",
        "X = co2.average\n",
        "train, test = X[1:len(X)-24], X[len(X)-24:]\n",
        "\n",
        "# train autoregression\n",
        "model = AutoReg(train, lags=20)\n",
        "model_fit = model.fit()\n",
        "print('Coefficients: %s' % model_fit.params)\n",
        "\n",
        "# Predictions\n",
        "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
        "\n",
        "# plot results\n",
        "plt.plot(test)\n",
        "plt.plot(predictions, color='red')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### ARMA (Autoregressive Moving Average)\n",
        "\n",
        "The ARMA model is a cornerstone of time series analysis and combines two major components: autoregressive (AR) and moving average (MA). It's designed to model time series data that is stationary, meaning the series has a constant mean and variance over time, and its covariance is independent of the time at which the series is observed.\n",
        "\n",
        "#### Components of ARMA:\n",
        "\n",
        "- **Autoregressive (AR) part** ($p$): This component models the current value of the time series as a linear combination of its previous values. The parameter $p$ represents the order of the AR part, indicating the number of lagged observations included in the model.\n",
        "\n",
        "\\begin{align}\n",
        "  AR(p): Y_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\dots + \\phi_p Y_{t-p} + \\epsilon_t\n",
        "\\end{align}\n",
        "\n",
        "- **Moving Average (MA) part** ($q$): This component models the current value of the series as a linear combination of the past error terms (the differences between past observations and predictions). The parameter $q$ represents the order of the MA part, indicating the number of lagged error terms included.\n",
        "\\begin{align}  \n",
        "  MA(q): Y_t = \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_q \\epsilon_{t-q}\n",
        "\\end{align}\n",
        "- **Combining AR and MA** gives the ARMA model, which can be written as:\n",
        "\\begin{align}\n",
        "  ARMA(p, q): Y_t = \\phi_1 Y_{t-1} + \\dots + \\phi_p Y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\dots + \\theta_q \\epsilon_{t-q}\n",
        "\\end{align}\n",
        "\n",
        "### ARIMA (Autoregressive Integrated Moving Average)\n",
        "\n",
        "ARIMA extends the ARMA model to include integration (I), making it suitable for analyzing non-stationary time series. The integration part involves differencing the time series one or more times to make it stationary.\n",
        "\n",
        "#### Components of ARIMA:\n",
        "\n",
        "- **Autoregressive (AR) part** ($p$): Same as in ARMA, it models the current value as a function of its previous values.\n",
        "- **Integrated (I) part** ($d$): This represents the order of differencing required to make the series stationary. Differencing is the process of subtracting the current observation from the previous observation. If $d$ differencing operations are required to achieve stationarity, the model is said to be integrated of order $d$.\n",
        "- **Moving Average (MA) part** ($q$): As with ARMA, it models the current value using the past forecast errors.\n",
        "\n",
        "The ARIMA model can thus be represented as ARIMA($p, d, q$), where:\n",
        "- $p$ = order of the autoregressive part,\n",
        "- $d$ = degree of first differencing involved,\n",
        "- $q$ = order of the moving average part.\n",
        "\n",
        "#### Formula:\n",
        "\\begin{align}\n",
        "\\Delta^d Y_t = \\phi_1 \\Delta^d Y_{t-1} + \\dots + \\phi_p \\Delta^d Y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\dots + \\theta_q \\epsilon_{t-q}\n",
        "\\end{align}\n",
        "where $\\Delta^d Y_t$ represents the $d$-times differenced series.\n",
        "\n",
        "### Usage and Applications\n",
        "\n",
        "- **ARMA** is best suited for stationary time series without trend and seasonal components.\n",
        "- **ARIMA** is versatile and can handle a wide range of time series, including those with trends but not seasonal patterns.\n",
        "\n",
        "For seasonal data, an extension of ARIMA known as Seasonal ARIMA (SARIMA) is often used, which incorporates seasonal differencing along with additional seasonal AR and MA terms.\n",
        "\n",
        "These models are fundamental in forecasting, allowing for predictions based on past values and the errors associated with those past predictions. Correctly specifying the $p$, $d$, and $q$ parameters is crucial for the success of these models, often achieved through iterative testing and model selection criteria such as AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion).\n",
        "\n",
        "### Example of ARIMA\n",
        "```python\n",
        "# ARIMA\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "model = ARIMA(co2.average, order=(10, 2, 3))\n",
        "# the order parameters:\n",
        "# p: No. of lag observations.\n",
        "# d: No. of times that the raw observations are differenced.\n",
        "# q: the size of the moving average window\n",
        "results_ARIMA = model.fit()\n",
        "results_ARIMA.summary()\n",
        "results_ARIMA.plot_predict(start=700)\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "pBlpMFaCW_LD"
      }
    }
  ]
}